{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "LABELS = {\n",
    "    'No Finding': 0,\n",
    "    'Atelectasis': 1,\n",
    "    'Cardiomegaly': 2,\n",
    "    'Consolidation': 3,\n",
    "    'Edema': 4,\n",
    "    'Effusion': 5,\n",
    "    'Emphysema': 6,\n",
    "    'Fibrosis': 7,\n",
    "    'Hernia': 8,\n",
    "    'Infiltration': 9,\n",
    "    'Infiltrate': 9,\n",
    "    'Mass': 10,\n",
    "    'Nodule': 11,\n",
    "    'Pleural_Thickening': 12,\n",
    "    'Pneumonia': 13,\n",
    "    'Pneumothorax': 14\n",
    "}\n",
    "\n",
    "PRIORS = np.array([[113.26, 94.50], [673.04, 494.74], [216.05, 218.86],\n",
    "                   [495.50, 343.66], [239.74, 490.62]])\n",
    "\n",
    "# reduce to X by X\n",
    "TARGET_GRID = 8\n",
    "CLASSES = 15\n",
    "ORIGINAL = 1024\n",
    "BLOCKSIZE = ORIGINAL // TARGET_GRID\n",
    "BOXES = 5\n",
    "\n",
    "data = {}\n",
    "with open('data/BBox_List_2017.csv') as f:\n",
    "    re = csv.reader(f)\n",
    "    next(re)\n",
    "    for r in re:\n",
    "        id = r[0]\n",
    "        if id not in data:\n",
    "            data[id] = np.zeros([TARGET_GRID, TARGET_GRID, BOXES, 5 + CLASSES])\n",
    "        obs = LABELS[r[1]] + 5\n",
    "\n",
    "        x = float(r[2])\n",
    "        x_block = int(x // BLOCKSIZE)\n",
    "        x_offset = (x % BLOCKSIZE) / BLOCKSIZE\n",
    "\n",
    "        y = float(r[3])\n",
    "        y_block = int(y // BLOCKSIZE)\n",
    "        y_offset = (y % BLOCKSIZE) / BLOCKSIZE\n",
    "\n",
    "        # exp(pred_w) * prior = w\n",
    "        w = float(r[4])\n",
    "        w = w / PRIORS[:, 0]\n",
    "        h = float(r[5])\n",
    "        h = h / PRIORS[:, 1]\n",
    "\n",
    "        # print(x, y)\n",
    "        # print(x_block, y_block)\n",
    "        # print(x_offset, y_offset)\n",
    "\n",
    "        # conf\n",
    "        data[id][x_block, y_block, :, 0] = 1\n",
    "        # x y\n",
    "        data[id][x_block, y_block, :, 1] = x_offset\n",
    "        data[id][x_block, y_block, :, 2] = y_offset\n",
    "        # w h\n",
    "        data[id][x_block, y_block, :, 3] = w\n",
    "        data[id][x_block, y_block, :, 4] = h\n",
    "        # classes\n",
    "        data[id][x_block, y_block, :, obs] = 1\n",
    "\n",
    "\n",
    "with open('data/labels_boxed.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8, 5, 20)\n"
     ]
    }
   ],
   "source": [
    "with open('data/labels_boxed.pkl', 'rb') as f:\n",
    "    data_labels_boxed =  pickle.load(f)\n",
    "print(data_labels_boxed['00013118_008.png'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    " \n",
    "LABELS = {\n",
    "    'No Finding': 0,\n",
    "    'Atelectasis': 1,\n",
    "    'Cardiomegaly': 2,\n",
    "    'Consolidation': -1,\n",
    "    'Edema': -1,\n",
    "    'Effusion': 3,\n",
    "    'Emphysema': -1,\n",
    "    'Fibrosis': -1,\n",
    "    'Hernia': -1,\n",
    "    'Infiltration': -1,\n",
    "    'Infiltrate': 4,\n",
    "    'Mass': 5,\n",
    "    'Nodule': 6,\n",
    "    'Pleural_Thickening': -1,\n",
    "    'Pneumonia': 7,\n",
    "    'Pneumothorax': 8\n",
    "}\n",
    " \n",
    "testset = set()\n",
    "validset = set()\n",
    "trainset = set()\n",
    " \n",
    "with open('data/train.txt') as f:\n",
    "    re = csv.reader(f)\n",
    "    for r in re:\n",
    "        trainset.add(r[0])\n",
    "with open('data/valid.txt') as f:\n",
    "    re = csv.reader(f)\n",
    "    for r in re:\n",
    "        validset.add(r[0])\n",
    "with open('data/test.txt') as f:\n",
    "    re = csv.reader(f)\n",
    "    for r in re:\n",
    "        testset.add(r[0])\n",
    "traindata = {}\n",
    "validdata = {}\n",
    "testdata = {}\n",
    " \n",
    "with open('data/Data_Entry_2017_v2.csv') as f:\n",
    "    re = csv.reader(f)\n",
    "    next(re)\n",
    "    for r in re:\n",
    "        id = r[0]\n",
    "        obs = []\n",
    "        for observe in r[1].split('|'):\n",
    "            ob = LABELS[observe]\n",
    "            if ob == -1:\n",
    "                continue\n",
    "            obs.append(ob)\n",
    "        if len(obs) == 0:\n",
    "            continue\n",
    "        if id in trainset:\n",
    "            traindata[id] = obs\n",
    "        elif id in validset:\n",
    "            validdata[id] = obs\n",
    "        elif id in testset:\n",
    "            testdata[id] = obs\n",
    "with open('data/pickles/labels_train.pkl', 'wb') as f:\n",
    "    pickle.dump(traindata, f)\n",
    "with open('data/pickles/labels_valid.pkl', 'wb') as f:\n",
    "    pickle.dump(validdata, f)\n",
    "with open('data/pickles/labels_test.pkl', 'wb') as f:\n",
    "    pickle.dump(testdata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, BatchNormalization, Input, Activation\n",
    "from keras.layers import LeakyReLU, Lambda, Reshape, Concatenate, Add, Dense\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "# from skimage.io import imread\n",
    "# from skimage import img_as_float\n",
    "# from skimage.transform import rescale\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "TRAINSIZE = 111240\n",
    "\n",
    "EPOCHS = 20\n",
    "STEPSPEREPOCH = 1024\n",
    "VALIDATIONSTEPS = 2\n",
    "BATCHSIZE = 64\n",
    "\n",
    "INPUT_SIZE = 256\n",
    "\n",
    "OUTPUT_GRID = 8\n",
    "BOXES = 5\n",
    "CLASSES = 15\n",
    "FINAL_DIMS = BOXES * (5 + CLASSES)\n",
    "\n",
    "MODELDIR = 'model/' + os.path.basename(os.path.splitext(sys.argv[0])[0])\n",
    "MODELFILE = MODELDIR + '/model'\n",
    "\n",
    "tb = TensorBoard(log_dir=MODELDIR)\n",
    "sv = ModelCheckpoint(MODELFILE, save_best_only=True, save_weights_only=True)\n",
    "es = EarlyStopping(patience=3)\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = create_model()\n",
    "\n",
    "    def g():\n",
    "        X, Y, W = [], [], []\n",
    "        while True:\n",
    "            if len(X) < BATCHSIZE * 0.1:\n",
    "                x, y, w = next(boxedgen())\n",
    "            else:\n",
    "                x, y, w = next(labelgen())\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            W.append(w)\n",
    "            if len(X) == BATCHSIZE:\n",
    "                Y = np.stack(Y)\n",
    "                Y = [Y[..., 0:1], Y[..., 1:3], Y[..., 3:5], Y[..., 5:]]\n",
    "                yield np.stack(X), Y, [\n",
    "                    np.squeeze(w) for w in np.hsplit(np.array(W), 4)\n",
    "                ]\n",
    "                X, Y, W = [], [], []\n",
    "\n",
    "    def vg():\n",
    "        X, Y, W = [], [], []\n",
    "        while True:\n",
    "            x, y, w = next(boxedgen())\n",
    "            # x, y = next(validgen())\n",
    "            # w = [0, 0, 0, 1]\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            W.append(w)\n",
    "            if len(X) == BATCHSIZE:\n",
    "                Y = np.stack(Y)\n",
    "                Y = [Y[..., 0:1], Y[..., 1:3], Y[..., 3:5], Y[..., 5:]]\n",
    "                yield np.stack(X), Y, [\n",
    "                    np.squeeze(w) for w in np.hsplit(np.array(W), 4)\n",
    "                ]\n",
    "                X, Y, W = [], [], []\n",
    "\n",
    "    model.fit_generator(\n",
    "        g(),\n",
    "        steps_per_epoch=STEPSPEREPOCH,\n",
    "        validation_data=vg(),\n",
    "        validation_steps=VALIDATIONSTEPS,\n",
    "        epochs=EPOCHS,\n",
    "        workers=3,\n",
    "        use_multiprocessing=True,\n",
    "        shuffle=True,\n",
    "        callbacks=[tb, sv, es])\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    def conv(x, filters, strides=1):\n",
    "        x = Conv2D(filters, 3, strides=strides, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        return x\n",
    "\n",
    "    def space_to_depth(x):\n",
    "        ss = x.shape[1] // 2\n",
    "        ldim = x.shape[-1]\n",
    "        x = K.reshape(x, [-1, ss, ss, ss, ss, ldim])\n",
    "        x = K.permute_dimensions(x, [0, 1, 3, 2, 4, 5])\n",
    "        x = K.reshape(x, [-1, ss, ss, ldim * 4])\n",
    "        return x\n",
    "\n",
    "    xi = Input([INPUT_SIZE, INPUT_SIZE])\n",
    "\n",
    "    x = Reshape([INPUT_SIZE, INPUT_SIZE, 1])(xi)\n",
    "\n",
    "    x = conv(x, 32)\n",
    "    x = conv(x, 32, 2)\n",
    "\n",
    "    x = conv(x, 32)\n",
    "    x = conv(x, 32, 2)\n",
    "\n",
    "    x = conv(x, 64)\n",
    "    x = conv(x, 64, 2)\n",
    "\n",
    "    x = conv(x, 64)\n",
    "    x = conv(x, 64, 2)\n",
    "\n",
    "    passthrough = Lambda(space_to_depth)(x)\n",
    "\n",
    "    x = conv(x, 128)\n",
    "    x = conv(x, 128, 2)\n",
    "    x = conv(x, 128)\n",
    "\n",
    "    x = Concatenate()([x, passthrough])\n",
    "\n",
    "    x = conv(x, 256)\n",
    "    x = conv(x, 256)\n",
    "\n",
    "    x = Conv2D(FINAL_DIMS, 3, padding='same')(x)\n",
    "    x = Reshape([OUTPUT_GRID, OUTPUT_GRID, BOXES, 5 + CLASSES])(x)\n",
    "\n",
    "    # ============ Output\n",
    "    conf = Lambda(lambda x: x[:, :, :, :, 0:1])(x)\n",
    "    conf = Activation('sigmoid', name='c')(conf)\n",
    "\n",
    "    xy = Lambda(lambda x: x[:, :, :, :, 1:3])(x)\n",
    "    xy = Activation('sigmoid', name='xy')(xy)\n",
    "\n",
    "    # wh = Lambda(lambda x: K.exp(x[:, :, :, :, 3:5]))(x)\n",
    "    wh = Lambda(lambda x: x[:, :, :, :, 3:5])(x)\n",
    "    wh = Activation('tanh')(wh)\n",
    "    ones = Lambda(lambda x: K.ones_like(x))(wh)\n",
    "    wh = Add(name='wh')([wh, ones])\n",
    "\n",
    "    prob = Lambda(lambda x: x[:, :, :, :, 5:])(x)\n",
    "    prob = Activation('sigmoid', name='p')(prob)\n",
    "\n",
    "    # ============ Model\n",
    "    model = Model(xi, [conf, xy, wh, prob])\n",
    "    model.compile(\n",
    "        'adam', ['binary_crossentropy', 'mse', 'mse', 'binary_crossentropy'],\n",
    "        loss_weights=[1, 5, 5, 1])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def boxedgen():\n",
    "    with open('data/pickles/labels_boxed.pkl', 'rb') as f:\n",
    "        boxed = pickle.load(f)\n",
    "    while True:\n",
    "        for k, v in boxed.items():\n",
    "            yield img_as_float(rescale(imread('data/images/' + k),\n",
    "                                       0.25)), v, [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "\n",
    "def labelgen():\n",
    "    with open('data/pickles/labels_train.pkl', 'rb') as f:\n",
    "        traindata = pickle.load(f)\n",
    "    while True:\n",
    "        for k, v in traindata.items():\n",
    "            lab = np.zeros([OUTPUT_GRID, OUTPUT_GRID, BOXES, 5 + CLASSES])\n",
    "            for obs in v:\n",
    "                lab[:, :, :, obs] = 1\n",
    "            yield img_as_float(rescale(imread('data/images/' + k),\n",
    "                                       0.25)), lab, [0, 0, 0, 1]\n",
    "\n",
    "\n",
    "def validgen():\n",
    "    with open('data/pickles/labels_valid.pkl', 'rb') as f:\n",
    "        traindata = pickle.load(f)\n",
    "    while True:\n",
    "        for k, v in traindata.items():\n",
    "            lab = np.zeros([OUTPUT_GRID, OUTPUT_GRID, BOXES, 5 + CLASSES])\n",
    "            for obs in v:\n",
    "                lab[:, :, :, obs] = 1\n",
    "            yield img_as_float(rescale(imread('data/images/' + k),\n",
    "                                       0.25)), lab, [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "def create_model_res():\n",
    "\n",
    "    xi = Input([INPUT_SIZE, INPUT_SIZE, 3])\n",
    "    x = Reshape([INPUT_SIZE, INPUT_SIZE, 3])(xi)\n",
    "\n",
    "    ir = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=x,\n",
    "        input_shape=(INPUT_SIZE, INPUT_SIZE, 3),\n",
    "        pooling='avg')\n",
    "\n",
    "    p = Dense(CLASSES, activation='softmax')(ir.output)\n",
    "\n",
    "    # ============ Model\n",
    "    model = Model(ir.input, p)\n",
    "    model.compile('adam', 'binary_crossentropy')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dense' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fa700b57c53f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-4de8a0e5bb25>\u001b[0m in \u001b[0;36mcreate_model_res\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         pooling='avg')\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# ============ Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dense' is not defined"
     ]
    }
   ],
   "source": [
    "model = create_model_res()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "CLASSES = 15\n",
    "def create_model_ResNetV2():\n",
    "    INPUT_SIZE = 256\n",
    "    xi = Input([INPUT_SIZE, INPUT_SIZE, 3 ])\n",
    "    x = Reshape([INPUT_SIZE, INPUT_SIZE, 3])(xi)\n",
    "\n",
    "    ir = InceptionResNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=x,\n",
    "        input_shape=(INPUT_SIZE, INPUT_SIZE, 3),\n",
    "        pooling='avg')\n",
    "\n",
    "    p = Dense(CLASSES, activation='softmax')(ir.output)\n",
    "\n",
    "    # ============ Model\n",
    "    model = Model(ir.input, p)\n",
    "    model.compile('adam', 'binary_crossentropy')\n",
    "#     model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread, imsave, imresize\n",
    "from keras.utils import to_categorical\n",
    "CLASSES = 15\n",
    "\n",
    "def x_gen(batch_size, valid = False):\n",
    "    if(valid):\n",
    "        with open('data/pickles/labels_valid.pkl', 'rb') as f:\n",
    "            traindata = pickle.load(f)\n",
    "    else:\n",
    "        with open('data/pickles/labels_train.pkl', 'rb') as f:\n",
    "            traindata = pickle.load(f)\n",
    "    X, y = [], []\n",
    "    while True:\n",
    "        for k, v in traindata.items():\n",
    "            img = imread('data/images/' + k, mode ='RGB')\n",
    "            img = img / 255\n",
    "            X.append(imresize(img ,size=(256,256)))\n",
    "            y.append(v[0])\n",
    "            if(len(X) == batch_size):\n",
    "                y = to_categorical(y, num_classes=CLASSES)\n",
    "                X = np.array(X)\n",
    "                yield X,y\n",
    "                X, y = [], []\n",
    "\n",
    "def train():\n",
    "    model = create_model_ResNetV2()\n",
    "    model.fit_generator( x_gen(32, valid = False), validation_data = x_gen(32, valid = True),steps_per_epoch=10, epochs=50, metrics=['acc'], )\n",
    "    \n",
    "    \n",
    "train()\n",
    "     \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
